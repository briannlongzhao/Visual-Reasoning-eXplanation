{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8199f3d2-90d5-49b7-9559-3648984b512d",
   "metadata": {},
   "source": [
    "ACE library.\n",
    "\n",
    "Library for discovering and testing concept activation vectors. It contains\n",
    "ConceptDiscovery class that is able to discover the concepts belonging to one\n",
    "of the possible ResNet_pytorch labels of the ResNet_pytorch task of a network\n",
    "and calculate each concept's TCAV score.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd2df86-bc67-4049-92ca-66da9d8d1ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7261292d-20b0-422a-a0bb-c36f49e330c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-accecd75e3f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcurdir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mace_helpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\U\\r\\Visual-Reasoning-eXplanation\\src\\ace_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_Xception\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradcam\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTARGET_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import os,sys,inspect\n",
    "import scipy.stats as stats\n",
    "import skimage.segmentation as segmentation\n",
    "import sklearn.cluster as cluster\n",
    "import sklearn.metrics.pairwise as metrics\n",
    "from tcav import cav\n",
    "curdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "sys.path.insert(0,curdir)\n",
    "from ace_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddcd4e0-ab16-4194-9640-0bed0a392a88",
   "metadata": {},
   "source": [
    "Discovering and testing concepts of a class.\n",
    "\n",
    "For a trained network, it first discovers the concepts as areas of the iamges in the class and then calculates the TCAV score of each concept. It is also able to transform images from pixel space into concept space.\n",
    "\n",
    "Runs concept discovery for a given class in a trained model.\n",
    "\n",
    "For a trained ResNet_pytorch model, the ConceptDiscovery class first performs unsupervised concept discovery using examples of one of the classes in the network.\n",
    "\n",
    "Args:\\\n",
    "    model: A trained ResNet_pytorch model on which we run the concept discovery algorithm\\\n",
    "    target_class: Name of the one of the classes of the network\\\n",
    "    random_concept: A concept made of random images (used for statistical test) e.g. \"random500_199\"\\\n",
    "    bottlenecks: a list of bottleneck layers of the model for which the cocept discovery stage is performed\\\n",
    "    sess: Model's tensorflow session\\\n",
    "    source_dir: This directory that contains folders with images of network's classes.\\\n",
    "    activation_dir: directory to save computed activations\\\n",
    "    cav_dir: directory to save CAVs of discovered and random concepts\\\n",
    "    num_random_exp: Number of random counterparts used for calculating several CAVs and TCAVs for each concept (to make statistical testing possible.)\\\n",
    "    channel_mean: If true, for the unsupervised concept discovery the bottleneck activations are averaged over channels instead of using the whole acivation vector (reducing dimensionality)\\\n",
    "    max_imgs: maximum number of images in a discovered concept\\\n",
    "    min_imgs : minimum number of images in a discovered concept for the concept to be accepted\\\n",
    "    num_discovery_imgs: Number of images used for concept discovery. If None, will use max_imgs instead.\\\n",
    "    num_workers: if greater than zero, runs methods in parallel with num_workers parallel threads. If 0, no method is run in parallel threads.\\\n",
    "    average_image_value: The average value used for mean subtraction in the nework's preprocessing stage.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2eed5863-9d13-4c2f-955b-adea8049cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptDiscovery(object):\n",
    "    def __init__(self,\n",
    "               model,\n",
    "               target_class,\n",
    "               random_concept,\n",
    "               bottlenecks,\n",
    "               sess,\n",
    "               source_dir,\n",
    "               activation_dir,\n",
    "               cav_dir,\n",
    "               num_random_exp=2,\n",
    "               channel_mean=True,\n",
    "               max_imgs=40,\n",
    "               min_imgs=20,\n",
    "               num_discovery_imgs=40,\n",
    "               num_workers=20,\n",
    "               average_image_value=117\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.sess = sess\n",
    "        self.target_class = target_class\n",
    "        self.num_random_exp = num_random_exp\n",
    "        if isinstance(bottlenecks, str):\n",
    "            bottlenecks = [bottlenecks]\n",
    "        self.bottlenecks = bottlenecks\n",
    "        self.source_dir = source_dir\n",
    "        self.activation_dir = activation_dir\n",
    "        self.cav_dir = cav_dir\n",
    "        self.channel_mean = channel_mean\n",
    "        self.random_concept = random_concept\n",
    "        self.image_shape = model.get_image_shape()[:2]\n",
    "        self.max_imgs = max_imgs\n",
    "        self.min_imgs = min_imgs\n",
    "        if num_discovery_imgs is None:\n",
    "            num_discovery_imgs = max_imgs\n",
    "        self.num_discovery_imgs = num_discovery_imgs\n",
    "        self.num_workers = num_workers\n",
    "        self.average_image_value = average_image_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffc0984-8fbc-4f06-a341-0145fb881edf",
   "metadata": {},
   "source": [
    "Loads all colored images of a concept.\n",
    "\n",
    "Args:\\\n",
    "    concept: The name of the concept to be loaded\\\n",
    "    max_imgs: maximum number of images to be loaded\n",
    "\n",
    "Returns:\\\n",
    "Images of the desired concept or class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "531c30f8-db8e-48d8-814a-9b425e20a090",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to ConceptDiscovery\n",
    "def load_concept_imgs(self, concept, max_imgs=1000, compute_tcav = False):\n",
    "    if compute_tcav:\n",
    "        concept_dir = os.path.join(self.source_dir, concept+'_50')\n",
    "    else:\n",
    "        concept_dir = os.path.join(self.source_dir, concept)\n",
    "\n",
    "    img_paths = [\n",
    "        os.path.join(concept_dir, d)\n",
    "        for d in tf.gfile.ListDirectory(concept_dir)\n",
    "    ]\n",
    "    return load_images_from_files(\n",
    "        img_paths,\n",
    "        max_imgs=max_imgs,\n",
    "        return_filenames=False,\n",
    "        do_shuffle=False,\n",
    "        run_parallel=(self.num_workers > 0),\n",
    "        shape=(self.image_shape),\n",
    "        num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74682477-fd13-42fd-8fc5-e7a6cb417895",
   "metadata": {},
   "source": [
    "Creates a set of image patches using superpixel methods.\n",
    "\n",
    "This method takes in the concept discovery images and transforms it to a dataset made of the patches of those images.\n",
    "\n",
    "Args:\\\n",
    "    method: The superpixel method used for creating image patches. One of 'slic', 'watershed', 'quickshift', 'felzenszwalb'.\\\n",
    "    discovery_images: Images used for creating patches. If None, the images in the target class folder are used.\\\n",
    "    param_dict: Contains parameters of the superpixel method used in the form of {'param1':[a,b,...], 'param2':[z,y,x,...], ...}. For instance {'n_segments':[15,50,80], 'compactness':[10,10,10]} for slic method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44f9ae7-03d3-41ac-83a3-4dd6d7a75187",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to ConceptDiscovery\n",
    "def create_patches(self, method='slic', discovery_images=None, param_dict=None, gradcam=False, keep_percent=80):\n",
    "    if param_dict is None:\n",
    "        param_dict = {}\n",
    "    dataset, image_numbers, patches = [], [], []\n",
    "    if discovery_images is None:\n",
    "        raw_imgs = self.load_concept_imgs(self.target_class, self.num_discovery_imgs, compute_tcav=True)\n",
    "        self.discovery_images = raw_imgs\n",
    "    else:\n",
    "        self.discovery_images = discovery_images\n",
    "    if self.num_workers:\n",
    "        pool = multiprocessing.Pool(self.num_workers)\n",
    "        if gradcam:\n",
    "            outputs = pool.map(lambda img: self._return_gradcam_superpixels(img, method, param_dict, keep_percent), self.discovery_images)\n",
    "        else:\n",
    "            outputs = pool.map(lambda img: self._return_superpixels(img, method, param_dict), self.discovery_images)\n",
    "        for fn, sp_outputs in enumerate(outputs):\n",
    "            image_superpixels, image_patches = sp_outputs\n",
    "            for superpixel, patch in zip(image_superpixels, image_patches):\n",
    "                dataset.append(superpixel)\n",
    "                patches.append(patch)\n",
    "                image_numbers.append(fn)\n",
    "    else:\n",
    "        for fn, img in enumerate(self.discovery_images):\n",
    "            if gradcam:\n",
    "                image_superpixels, image_patches = self._return_gradcam_superpixels(img, method, param_dict, keep_percent)\n",
    "            else:\n",
    "                image_superpixels, image_patches = self._return_superpixels(img, method, param_dict)\n",
    "            for superpixel, patch in zip(image_superpixels, image_patches):\n",
    "                dataset.append(superpixel)\n",
    "                patches.append(patch)\n",
    "                image_numbers.append(fn)\n",
    "    self.dataset, self.image_numbers, self.patches = np.array(dataset), np.array(image_numbers), np.array(patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593cc581-2d32-43c9-a811-95d541864562",
   "metadata": {},
   "source": [
    "Returns all patches for one image using gradcam.\n",
    "\n",
    "Given an image, calculates superpixels for each of the parameter lists in param_dict and returns a set of unique superpixels by removing duplicates. If two patches have Jaccard similarity more than 0.5, they are concidered duplicates.\n",
    "\n",
    "Args:\n",
    "\n",
    "img: The input image \\\n",
    "method: superpixel method, one of slic, watershed, quichsift, or felzenszwalb \\\n",
    "param_dict: Contains parameters of the superpixel method used in the form of {'param1':[a,b,...], 'param2':[z,y,x,...], ...}. For instance {'n_segments':[15,50,80], 'compactness':[10,10,10]} for slic method.\n",
    "    \n",
    "Raises:\n",
    "\n",
    "ValueError: if the segementation method is invaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c01be-41dc-4303-b39d-a6c46b5ffd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to ConceptDiscovery\n",
    "def _return_gradcam_superpixels(self, img, method='slic', param_dict=None, keep_percent=80):\n",
    "    if param_dict is None:\n",
    "        param_dict = {}\n",
    "\n",
    "    upsample_size = (img.shape[1], img.shape[0])\n",
    "    gradCAM = self.model.gradCAM_model\n",
    "    chosen_class = int(self.model.labels[self.target_class]) #####\n",
    "    inputs = np.expand_dims(img, axis=0)\n",
    "    # ori_img = (img * 256).astype(np.uint8)\n",
    "    # gradCAM.showCAMs(ori_img, inputs, chosen_class, upsample_size)\n",
    "    cam = gradCAM.compute_heatmap(inputs, chosen_class, upsample_size, keep_percent)[1][:,:,0]\n",
    "\n",
    "    if method == 'slic':\n",
    "        n_segmentss = param_dict.pop('n_segments', [15, 50, 80])\n",
    "        n_params = len(n_segmentss)\n",
    "        compactnesses = param_dict.pop('compactness', [20] * n_params)\n",
    "        sigmas = param_dict.pop('sigma', [1.] * n_params)\n",
    "    elif method == 'watershed':\n",
    "        markerss = param_dict.pop('marker', [15, 50, 80])\n",
    "        n_params = len(markerss)\n",
    "        compactnesses = param_dict.pop('compactness', [0.] * n_params)\n",
    "    elif method == 'quickshift':\n",
    "        max_dists = param_dict.pop('max_dist', [20, 15, 10])\n",
    "        n_params = len(max_dists)\n",
    "        ratios = param_dict.pop('ratio', [1.0] * n_params)\n",
    "        kernel_sizes = param_dict.pop('kernel_size', [10] * n_params)\n",
    "    elif method == 'felzenszwalb':\n",
    "        scales = param_dict.pop('scale', [1200, 500, 250])\n",
    "        n_params = len(scales)\n",
    "        sigmas = param_dict.pop('sigma', [0.8] * n_params)\n",
    "        min_sizes = param_dict.pop('min_size', [20] * n_params)\n",
    "    else:\n",
    "        raise ValueError('Invalid superpixel method!')\n",
    "    unique_masks = []\n",
    "    for i in range(n_params):\n",
    "        param_masks = []\n",
    "        if method == 'slic':\n",
    "        segments = segmentation.slic(\n",
    "            img, n_segments=n_segmentss[i], compactness=compactnesses[i],\n",
    "            sigma=sigmas[i])\n",
    "        elif method == 'watershed':\n",
    "        segments = segmentation.watershed(\n",
    "            img, markers=markerss[i], compactness=compactnesses[i])\n",
    "        elif method == 'quickshift':\n",
    "        segments = segmentation.quickshift(\n",
    "            img, kernel_size=kernel_sizes[i], max_dist=max_dists[i],\n",
    "            ratio=ratios[i])\n",
    "        elif method == 'felzenszwalb':\n",
    "        segments = segmentation.felzenszwalb(\n",
    "            img, scale=scales[i], sigma=sigmas[i], min_size=min_sizes[i])\n",
    "        for s in range(segments.max()):\n",
    "            mask = (segments == s).astype(float)\n",
    "            if np.mean(mask) > 0.001:\n",
    "                unique = True\n",
    "\n",
    "                if sum(sum(cam * mask)) == sum(sum(mask)):\n",
    "                    for seen_mask in unique_masks:\n",
    "                        jaccard = np.sum(seen_mask * mask) / np.sum((seen_mask + mask) > 0)\n",
    "                        if jaccard > 0.5:\n",
    "                            unique = False\n",
    "                            break\n",
    "                else:\n",
    "                    unique = False\n",
    "\n",
    "                if unique:\n",
    "                    param_masks.append(mask)\n",
    "        unique_masks.extend(param_masks)\n",
    "    superpixels, patches = [], []\n",
    "    while unique_masks:\n",
    "        superpixel, patch = self._extract_patch(img, unique_masks.pop())\n",
    "        superpixels.append(superpixel)\n",
    "        patches.append(patch)\n",
    "    return superpixels, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce6b89-5d9f-491c-a9f2-4d5b95b37b86",
   "metadata": {},
   "source": [
    "Returns all patches for one image.\n",
    "\n",
    "Given an image, calculates superpixels for each of the parameter lists in param_dict and returns a set of unique superpixels by removing duplicates. If two patches have Jaccard similarity more than 0.5, they are concidered duplicates.\n",
    "\n",
    "Args:\n",
    "\n",
    "img: The input image \\\n",
    "method: superpixel method, one of slic, watershed, quichsift, or felzenszwalb \\\n",
    "param_dict: Contains parameters of the superpixel method used in the form of {'param1':[a,b,...], 'param2':[z,y,x,...], ...}. For instance {'n_segments':[15,50,80], 'compactness':[10,10,10]} for slic method.\n",
    "    \n",
    "Raises:\n",
    "\n",
    "ValueError: if the segementation method is invaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd1824-63ae-409a-95ad-1829adec1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to ConceptDiscovery\n",
    "def _return_superpixels(self, img, method='slic',\n",
    "                          param_dict=None):\n",
    "    if param_dict is None:\n",
    "      param_dict = {}\n",
    "\n",
    "    if method == 'slic':\n",
    "      n_segmentss = param_dict.pop('n_segments', [15, 50, 80])\n",
    "      n_params = len(n_segmentss)\n",
    "      compactnesses = param_dict.pop('compactness', [20] * n_params)\n",
    "      sigmas = param_dict.pop('sigma', [1.] * n_params)\n",
    "    elif method == 'watershed':\n",
    "      markerss = param_dict.pop('marker', [15, 50, 80])\n",
    "      n_params = len(markerss)\n",
    "      compactnesses = param_dict.pop('compactness', [0.] * n_params)\n",
    "    elif method == 'quickshift':\n",
    "      max_dists = param_dict.pop('max_dist', [20, 15, 10])\n",
    "      n_params = len(max_dists)\n",
    "      ratios = param_dict.pop('ratio', [1.0] * n_params)\n",
    "      kernel_sizes = param_dict.pop('kernel_size', [10] * n_params)\n",
    "    elif method == 'felzenszwalb':\n",
    "      scales = param_dict.pop('scale', [1200, 500, 250])\n",
    "      n_params = len(scales)\n",
    "      sigmas = param_dict.pop('sigma', [0.8] * n_params)\n",
    "      min_sizes = param_dict.pop('min_size', [20] * n_params)\n",
    "    else:\n",
    "      raise ValueError('Invalid superpixel method!')\n",
    "    unique_masks = []\n",
    "    for i in range(n_params):\n",
    "      param_masks = []\n",
    "      if method == 'slic':\n",
    "        segments = segmentation.slic(\n",
    "            img, n_segments=n_segmentss[i], compactness=compactnesses[i],\n",
    "            sigma=sigmas[i])\n",
    "      elif method == 'watershed':\n",
    "        segments = segmentation.watershed(\n",
    "            img, markers=markerss[i], compactness=compactnesses[i])\n",
    "      elif method == 'quickshift':\n",
    "        segments = segmentation.quickshift(\n",
    "            img, kernel_size=kernel_sizes[i], max_dist=max_dists[i],\n",
    "            ratio=ratios[i])\n",
    "      elif method == 'felzenszwalb':\n",
    "        segments = segmentation.felzenszwalb(\n",
    "            img, scale=scales[i], sigma=sigmas[i], min_size=min_sizes[i])\n",
    "      for s in range(segments.max()):\n",
    "        mask = (segments == s).astype(float)\n",
    "        if np.mean(mask) > 0.001:\n",
    "          unique = True\n",
    "          for seen_mask in unique_masks:\n",
    "            jaccard = np.sum(seen_mask * mask) / np.sum((seen_mask + mask) > 0)\n",
    "            if jaccard > 0.5:\n",
    "              unique = False\n",
    "              break\n",
    "          if unique:\n",
    "            param_masks.append(mask)\n",
    "      unique_masks.extend(param_masks)\n",
    "    superpixels, patches = [], []\n",
    "    while unique_masks:\n",
    "      superpixel, patch = self._extract_patch(img, unique_masks.pop())\n",
    "      superpixels.append(superpixel)\n",
    "      patches.append(patch)\n",
    "    return superpixels, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e42d25-aad6-4af5-a8bf-881d98ce114d",
   "metadata": {},
   "source": [
    "Extracts a patch out of an image.\n",
    "\n",
    "    Args:\n",
    "      image: The original image\n",
    "      mask: The binary mask of the patch area\n",
    "\n",
    "    Returns:\n",
    "      image_resized: The resized patch such that its boundaries touches the\n",
    "        image boundaries\n",
    "      patch: The original patch. Rest of the image is padded with average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f985f9d-39e1-457e-94c5-f1ff9f8921de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_patch(self, image, mask):\n",
    "    mask_expanded = np.expand_dims(mask, -1)\n",
    "    patch = (mask_expanded * image + (\n",
    "        1 - mask_expanded) * float(self.average_image_value) / 255)\n",
    "    ones = np.where(mask == 1)\n",
    "    h1, h2, w1, w2 = ones[0].min(), ones[0].max(), ones[1].min(), ones[1].max()\n",
    "    image = Image.fromarray((patch[h1:h2, w1:w2] * 255).astype(np.uint8))\n",
    "    image_resized = np.array(image.resize(self.image_shape, Image.BICUBIC)).astype(float) / 255\n",
    "    return image_resized, patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5faff09-93d3-41a8-b772-3480f462821d",
   "metadata": {},
   "source": [
    "    Returns activations of a list of imgs.\n",
    "\n",
    "    Args:\n",
    "      imgs: List/array of images to calculate the activations of\n",
    "      bottleneck: Name of the bottleneck layer of the model where activations\n",
    "        are calculated\n",
    "      bs: The batch size for calculating activations. (To control computational\n",
    "        cost)\n",
    "      channel_mean: If true, the activations are averaged across channel.\n",
    "\n",
    "    Returns:\n",
    "      The array of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbfcb6-a6d3-42a2-adec-8106b99980a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patch_activations(self, imgs, bottleneck, bs=100, channel_mean=None):\n",
    "    if channel_mean is None:\n",
    "      channel_mean = self.channel_mean\n",
    "    if self.num_workers:\n",
    "      pool = multiprocessing.Pool(self.num_workers)\n",
    "      output = pool.map(\n",
    "          lambda i: self.model.run_examples(imgs[i * bs:(i + 1) * bs], bottleneck),\n",
    "          np.arange(int(imgs.shape[0] / bs) + 1))\n",
    "    else:\n",
    "      output = []\n",
    "      for i in range(int(imgs.shape[0] / bs) + 1):\n",
    "        if imgs[i * bs:(i + 1) * bs].shape[0] > 0:\n",
    "          output.append(\n",
    "              self.model.run_examples(imgs[i * bs:(i + 1) * bs], bottleneck))\n",
    "    output = np.concatenate(output, 0)\n",
    "    if channel_mean and len(output.shape) > 3:\n",
    "      output = np.mean(output, (1, 2))\n",
    "    else:\n",
    "      output = np.reshape(output, [output.shape[0], -1])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d768179f-82a6-4183-b41e-e8c56a86a0fc",
   "metadata": {},
   "source": [
    "    Runs unsupervised clustering algorithm on concept actiavtations.\n",
    "\n",
    "    Args:\n",
    "      acts: activation vectors of datapoints points in the bottleneck layer.\n",
    "        E.g. (number of clusters,) for Kmeans\n",
    "      method: clustering method. We have:\n",
    "        'KM': Kmeans Clustering\n",
    "        'AP': Affinity Propagation\n",
    "        'SC': Spectral Clustering\n",
    "        'MS': Mean Shift clustering\n",
    "        'DB': DBSCAN clustering method\n",
    "      param_dict: Contains superpixl method's parameters. If an empty dict is\n",
    "                 given, default parameters are used.\n",
    "\n",
    "    Returns:\n",
    "      asg: The cluster assignment label of each data points\n",
    "      cost: The clustering cost of each data point\n",
    "      centers: The cluster centers. For methods like Affinity Propagetion\n",
    "      where they do not return a cluster center or a clustering cost, it\n",
    "      calculates the medoid as the center  and returns distance to center as\n",
    "      each data points clustering cost.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if the clustering method is invalid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb3011-ca89-4d4e-9ee4-a4b032969bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cluster(self, acts, method='KM', param_dict=None):\n",
    "    if param_dict is None:\n",
    "      param_dict = {}\n",
    "    centers = None\n",
    "    if method == 'KM':\n",
    "      n_clusters = param_dict.pop('n_clusters', 25)\n",
    "      km = cluster.KMeans(n_clusters)\n",
    "      d = km.fit(acts)\n",
    "      centers = km.cluster_centers_\n",
    "      d = np.linalg.norm(\n",
    "          np.expand_dims(acts, 1) - np.expand_dims(centers, 0), ord=2, axis=-1)\n",
    "      asg, cost = np.argmin(d, -1), np.min(d, -1)\n",
    "    elif method == 'AP':\n",
    "      damping = param_dict.pop('damping', 0.5)\n",
    "      ca = cluster.AffinityPropagation(damping)\n",
    "      ca.fit(acts)\n",
    "      centers = ca.cluster_centers_\n",
    "      d = np.linalg.norm(\n",
    "          np.expand_dims(acts, 1) - np.expand_dims(centers, 0), ord=2, axis=-1)\n",
    "      asg, cost = np.argmin(d, -1), np.min(d, -1)\n",
    "    elif method == 'MS':\n",
    "      ms = cluster.MeanShift(n_jobs=self.num_workers)\n",
    "      asg = ms.fit_predict(acts)\n",
    "    elif method == 'SC':\n",
    "      n_clusters = param_dict.pop('n_clusters', 25)\n",
    "      sc = cluster.SpectralClustering(\n",
    "          n_clusters=n_clusters, n_jobs=self.num_workers)\n",
    "      asg = sc.fit_predict(acts)\n",
    "    elif method == 'DB':\n",
    "      eps = param_dict.pop('eps', 0.5)\n",
    "      min_samples = param_dict.pop('min_samples', 20)\n",
    "      sc = cluster.DBSCAN(eps, min_samples, n_jobs=self.num_workers)\n",
    "      asg = sc.fit_predict(acts)\n",
    "    else:\n",
    "      raise ValueError('Invalid Clustering Method!')\n",
    "    if centers is None:  ## If clustering returned cluster centers, use medoids\n",
    "      centers = np.zeros((asg.max() + 1, acts.shape[1]))\n",
    "      cost = np.zeros(len(acts))\n",
    "      for cluster_label in range(asg.max() + 1):\n",
    "        cluster_idxs = np.where(asg == cluster_label)[0]\n",
    "        cluster_points = acts[cluster_idxs]\n",
    "        pw_distances = metrics.euclidean_distances(cluster_points)\n",
    "        centers[cluster_label] = cluster_points[np.argmin(\n",
    "            np.sum(pw_distances, -1))]\n",
    "        cost[cluster_idxs] = np.linalg.norm(\n",
    "            acts[cluster_idxs] - np.expand_dims(centers[cluster_label], 0),\n",
    "            ord=2,\n",
    "            axis=-1)\n",
    "    return asg, cost, centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9ba291-5443-432e-a1af-bab8789bbbe9",
   "metadata": {},
   "source": [
    "    Discovers the frequent occurring concepts in the target class.\n",
    "\n",
    "      Calculates self.dic, a dicationary containing all the informations of the\n",
    "      discovered concepts in the form of {'bottleneck layer name: bn_dic} where\n",
    "      bn_dic itself is in the form of {'concepts:list of concepts,\n",
    "      'concept name': concept_dic} where the concept_dic is in the form of\n",
    "      {'images': resized patches of concept, 'patches': original patches of the\n",
    "      concepts, 'image_numbers': image id of each patch}\n",
    "\n",
    "    Args:\n",
    "      method: Clustering method.\n",
    "      activations: If activations are already calculated. If not calculates\n",
    "                   them. Must be a dictionary in the form of {'bn':array, ...}\n",
    "      param_dicts: A dictionary in the format of {'bottleneck':param_dict,...}\n",
    "                   where param_dict contains the clustering method's parametrs\n",
    "                   in the form of {'param1':value, ...}. For instance for Kmeans\n",
    "                   {'n_clusters':25}. param_dicts can also be in the format\n",
    "                   of param_dict where same parameters are used for all\n",
    "                   bottlenecks.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf0a07-d759-4c06-9214-b4d0d80659d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_concepts(self,\n",
    "                        method='KM',\n",
    "                        activations=None,\n",
    "                        param_dicts=None):\n",
    "    if param_dicts is None:\n",
    "      param_dicts = {}\n",
    "    if set(param_dicts.keys()) != set(self.bottlenecks):\n",
    "      param_dicts = {bn: param_dicts for bn in self.bottlenecks}\n",
    "    self.dic = {}  ## The main dictionary of the ConceptDiscovery class.\n",
    "    for bn in self.bottlenecks:\n",
    "      bn_dic = {}\n",
    "      if activations is None or bn not in activations.keys():\n",
    "        bn_activations = self._patch_activations(self.dataset, bn)\n",
    "      else:\n",
    "        bn_activations = activations[bn]\n",
    "      bn_dic['label'], bn_dic['cost'], centers = self._cluster(\n",
    "          bn_activations, method, param_dicts[bn])\n",
    "      concept_number, bn_dic['concepts'] = 0, []\n",
    "      for i in range(bn_dic['label'].max() + 1):\n",
    "        label_idxs = np.where(bn_dic['label'] == i)[0]\n",
    "        if len(label_idxs) > self.min_imgs:\n",
    "          concept_costs = bn_dic['cost'][label_idxs]\n",
    "          concept_idxs = label_idxs[np.argsort(concept_costs)[:self.max_imgs]]\n",
    "          concept_image_numbers = set(self.image_numbers[label_idxs])\n",
    "          discovery_size = len(self.discovery_images)\n",
    "          highly_common_concept = len(\n",
    "              concept_image_numbers) > 0.5 * len(label_idxs)\n",
    "          mildly_common_concept = len(\n",
    "              concept_image_numbers) > 0.25 * len(label_idxs)\n",
    "          mildly_populated_concept = len(\n",
    "              concept_image_numbers) > 0.25 * discovery_size\n",
    "          cond2 = mildly_populated_concept and mildly_common_concept\n",
    "          non_common_concept = len(\n",
    "              concept_image_numbers) > 0.1 * len(label_idxs)\n",
    "          highly_populated_concept = len(\n",
    "              concept_image_numbers) > 0.5 * discovery_size\n",
    "          cond3 = non_common_concept and highly_populated_concept\n",
    "          if highly_common_concept or cond2 or cond3:\n",
    "            concept_number += 1\n",
    "            concept = '{}_concept{}'.format(self.target_class, concept_number)\n",
    "            bn_dic['concepts'].append(concept)\n",
    "            bn_dic[concept] = {\n",
    "                'images': self.dataset[concept_idxs],\n",
    "                'patches': self.patches[concept_idxs],\n",
    "                'image_numbers': self.image_numbers[concept_idxs]\n",
    "            }\n",
    "            bn_dic[concept + '_center'] = centers[i] # most important, we store the center of all 25 concept\n",
    "      bn_dic.pop('label', None)\n",
    "      bn_dic.pop('cost', None)\n",
    "      self.dic[bn] = bn_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa11d9b-877e-4bad-ad6d-a53a81303d60",
   "metadata": {},
   "source": [
    "Wrapper for computing or loading activations of random concepts.\n",
    "\n",
    "    Takes care of making, caching (if desired) and loading activations.\n",
    "\n",
    "    Args:\n",
    "      bottleneck: The bottleneck layer name\n",
    "      random_concept: Name of the random concept e.g. \"random500_0\"\n",
    "\n",
    "    Returns:\n",
    "      A nested dict in the form of {concept:{bottleneck:activation}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de21c3e6-72d1-4616-bb0f-7f04de2883e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_concept_activations(self, bottleneck, random_concept):\n",
    "    rnd_acts_path = os.path.join(self.activation_dir, 'acts_{}_{}'.format(\n",
    "        random_concept, bottleneck))\n",
    "    if not tf.gfile.Exists(rnd_acts_path):\n",
    "      rnd_imgs = self.load_concept_imgs(random_concept, self.max_imgs)\n",
    "      acts = get_acts_from_images(rnd_imgs, self.model, bottleneck)\n",
    "      with tf.gfile.Open(rnd_acts_path, 'w') as f:\n",
    "        np.save(f, acts, allow_pickle=False)\n",
    "      del acts\n",
    "      del rnd_imgs\n",
    "    return np.load(rnd_acts_path).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
