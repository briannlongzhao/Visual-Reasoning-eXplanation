{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a054fb-a137-41f8-8af1-e12250c44fc1",
   "metadata": {},
   "source": [
    "This script use trained model explain Why and Why Not for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8c8478e-72fd-4081-a416-4f060c221ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate 3.6\n",
    "import jdc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2173f-3238-495c-90af-11c78ea25aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "import torch.nn.functional as f\n",
    "import heapq\n",
    "import matplotlib.image as mpimg\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from src.VR_reasoning_Xception import MyGCNNet_shareW_adap_batch_withhook\n",
    "from scipy.special import softmax\n",
    "import sys\n",
    "import collections\n",
    "from pylab import arrow\n",
    "from tcav import utils\n",
    "import src.ace_helpers as ace_helpers\n",
    "from src.ace_match_dummy import ConceptDiscovery\n",
    "import matplotlib.gridspec as gridspec\n",
    "from PIL import Image\n",
    "from matplotlib import ticker, cm\n",
    "import matplotlib\n",
    "from torch.nn import Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7ff1e-181f-4c21-8e6f-68e34dc33387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn_backward(module, grad_input, grad_output): # get gradient\n",
    "    total_grad_out.append(grad_input) # W * X + b\n",
    "def hook_fn_forward(module, input, output): # get activation of X\n",
    "    total_activation_out.append(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60faae67-f258-4934-b9e5-b2b988d0c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoning(model_share, data_package, device, args):\n",
    "    model_share.train()\n",
    "    '''\n",
    "    graph information\n",
    "    '''\n",
    "    node_dim = 32\n",
    "    node_num = 4\n",
    "    edge_dim = 5\n",
    "    edge_num = 12\n",
    "    '''\n",
    "    class index\n",
    "    '''\n",
    "    class_list = ['fire_engine', 'ambulance', 'school_bus']\n",
    "    all_data = {}\n",
    "    # graph_output = [0]*len(class_list)\n",
    "    # data = data_package[1][2].to(device) # get the positive class\n",
    "    label = class_list.index((data_package[1][0]))+1 # 1, 2, 3\n",
    "    for pack in data_package[1:]:\n",
    "        cate, path, data = pack\n",
    "        all_data[cate.split('2')[-1]] = data\n",
    "\n",
    "    graph_1 = all_data['fire_engine'].to(device)\n",
    "    output_1 = model_share.forward_1(graph_1.x, graph_1.edge_attr, graph_1.edge_index, batch=1)\n",
    "    graph_2 = all_data['ambulance'].to(device)\n",
    "    output_2 = model_share.forward_2(graph_2.x, graph_2.edge_attr, graph_2.edge_index, batch=1)\n",
    "    graph_3 = all_data['school_bus'].to(device)\n",
    "    output_3 = model_share.forward_3(graph_3.x, graph_3.edge_attr, graph_3.edge_index, batch=1)\n",
    "    graph_predict = model_share.forward(output_1, output_2, output_3)  # MLP merge\n",
    "    # norm_graph_predict = f.normalize(graph_predict, p=1, dim=1)  # normalize\n",
    "    norm_graph_predict = softmax(graph_predict.cpu().detach().numpy())  # normalize\n",
    "    '''\n",
    "    Gradient analysis for reasoning\n",
    "    '''\n",
    "    # output.backward(torch.ones(output.shape).to(device)) # if we do as a whole, will mix all grad\n",
    "    for index in range(graph_predict.shape[-1]):\n",
    "        graph_predict[0][index].backward(retain_graph=True) # for each output, calculate the gradient\n",
    "    print('reasoning finished')\n",
    "\n",
    "    # Gradient analysis: all class's gradient are stroed in total_grad_out(3 in this case)\n",
    "    GT_gradient = total_grad_out[label - 1][1]  # 0, bias; 1, Grad_X (1, 188), 2, Grad_W\n",
    "    GT_activation = total_activation_out[0][0]  # 0, bias; 1, Grad_X (1, 188), 2, Grad_W\n",
    "    Attention = GT_activation * GT_gradient\n",
    "    # dim(32) * 4 + dim_edge_features(5) * 12\n",
    "    Attention = Attention.resize(3,1,188)\n",
    "\n",
    "    # answer why\n",
    "    Pos_Attention = Attention[label - 1]\n",
    "    visual_related = Pos_Attention[:, : node_dim * node_num].resize(node_num, node_dim)\n",
    "    visual_reason = np.sum(visual_related.cpu().detach().numpy(), axis=1)  # (4 each one for a node)\n",
    "    structure_related = Pos_Attention[:, node_dim * node_num:].resize(edge_num, edge_dim)\n",
    "    structure_reason = np.sum(structure_related.cpu().detach().numpy(), axis=1)  # (12 each one for a edge)\n",
    "    '''\n",
    "    Answer correct or not (based on original model output)\n",
    "    '''\n",
    "    reason_dict = collections.defaultdict(dict)\n",
    "    GT = data_package[0]\n",
    "    output_list = list(GT)\n",
    "    predict = output_list[0].argmax() + 1\n",
    "    if predict == label: # Correct, we answer why correct\n",
    "        print('\\nCorrect prediction: {}'.format(class_list[predict-1]), '\\n')\n",
    "        print('Why correct? positive node and edge are correct.\\n',\n",
    "              'Visual reason:{}'.format(visual_reason),'\\n',\n",
    "              'structure reason:{}\\n'.format(structure_reason))\n",
    "        reason_dict[class_list[predict-1]][\"visual_reason\"] = visual_reason\n",
    "        reason_dict[class_list[predict-1]][\"structure_reason\"] = structure_reason\n",
    "\n",
    "        with open(os.path.join(args.result_root, \"reason_results\", \"%s_%s.txt\" % (args.img_class, args.img_idx)), 'w') as f:\n",
    "            f.write('Correct prediction: {}\\n'.format(class_list[predict-1]))\n",
    "            f.write('Why correct? positive node and edge are correct.\\n\\n')\n",
    "            f.write('Visual reason:{}\\n'.format(visual_reason))\n",
    "            f.write('Structure reason:{}\\n\\n'.format(structure_reason))\n",
    "\n",
    "        print('\\nStart answering why not')\n",
    "        all_label = [1, 2, 3]\n",
    "        remain_label = all_label.copy()\n",
    "        remain_label.remove(label)\n",
    "        for neg_label in remain_label: # exclude positive class 0, keep only 1 and 2\n",
    "            negative_activation = Attention[neg_label - 1]  # 0, bias; 1, Grad_X (1, 188), 2, Grad_W\n",
    "            visual_related = negative_activation[:, : node_dim * node_num].resize(node_num, node_dim)\n",
    "            visual_reason = np.sum(visual_related.cpu().detach().numpy(), axis=1)  # (4 each one for a node)\n",
    "            structure_related = negative_activation[:, node_dim * node_num:].resize(edge_num, edge_dim)\n",
    "            structure_reason = np.sum(structure_related.cpu().detach().numpy(), axis=1)  # (12 each one for a edge)\n",
    "\n",
    "            reason_dict[class_list[neg_label-1]][\"visual_reason\"] = visual_reason\n",
    "            reason_dict[class_list[neg_label-1]][\"structure_reason\"] = structure_reason\n",
    "\n",
    "            print('Why not {}? negative node and edge explain.\\n'.format(class_list[neg_label-1]),\n",
    "                  'Visual reason:{}\\n'.format(visual_reason),\n",
    "                  'structure reason:{}\\n'.format(structure_reason))\n",
    "\n",
    "            with open(os.path.join(args.result_root, \"reason_results\", \"%s_%s.txt\" % (args.img_class, args.img_idx)), 'a') as f:\n",
    "                f.write('Why not {}? negative node and edge explain.\\n'.format(class_list[neg_label-1]))\n",
    "                f.write('Visual reason:{}\\n'.format(visual_reason))\n",
    "                f.write('structure reason:{}\\n\\n'.format(structure_reason))\n",
    "\n",
    "    else: # Wrong, we answer why wrong\n",
    "        print('Wrong prediction: should be {}, wrong predict as {}'.format(class_list[label-1],\n",
    "                                                                          class_list[predict-1]))\n",
    "        print('Why wrong? (1)for the GT class, negative node and edge are bad for GT class.\\n',\n",
    "              'Visual reason:{}\\n'.format(visual_reason),\n",
    "              'structure reason:{}\\n\\n'.format(structure_reason))\n",
    "\n",
    "        reason_dict[class_list[label-1]][\"visual_reason\"] = visual_reason\n",
    "        reason_dict[class_list[label-1]][\"structure_reason\"] = structure_reason\n",
    "\n",
    "        with open(os.path.join(args.result_root, \"reason_results\", \"%s_%s.txt\" % (args.img_class, args.img_idx)), 'w') as f:\n",
    "            f.write('Wrong prediction: should be {}, wrong predict as {}\\n'.format(class_list[label-1],\n",
    "                                                                          class_list[predict-1]))\n",
    "            f.write('Why wrong? (1)for the GT class, negative node and edge are bad for GT class.\\n\\n')\n",
    "            f.write('Visual reason:{}\\n'.format(visual_reason))\n",
    "            f.write('Structure reason:{}\\n\\n'.format(structure_reason))\n",
    "\n",
    "        negative_activation = Attention[predict - 1]  # 0, bias; 1, Grad_X (1, 188), 2, Grad_W\n",
    "        # dim(32) * 4 + dim_edge_features(5) * 12\n",
    "        visual_related = negative_activation[:, : node_dim * node_num].resize(node_num, node_dim)\n",
    "        # visual_reason = np.mean(visual_related.cpu().detach().numpy(), axis=1) #(4 each one for a node)\n",
    "        visual_reason = np.sum(visual_related.cpu().detach().numpy(), axis=1)  # (4 each one for a node)\n",
    "        structure_related = negative_activation[:, node_dim * node_num:].resize(edge_num, edge_dim)\n",
    "        structure_reason = np.sum(structure_related.cpu().detach().numpy(), axis=1)  # (12 each one for a edge)\n",
    "        print('Why wrong? (2)for the Predicted class, positive node and edge are bad for Predicted class .\\n',\n",
    "              'Visual reason:{}'.format(visual_reason), '\\n',\n",
    "              'structure reason:{}'.format(structure_reason))\n",
    "\n",
    "        reason_dict[class_list[predict-1]][\"visual_reason\"] = visual_reason\n",
    "        reason_dict[class_list[predict-1]][\"structure_reason\"] = structure_reason\n",
    "\n",
    "        with open(os.path.join(args.result_root, \"reason_results\", \"%s_%s.txt\" % (args.img_class, args.img_idx)), 'a') as f:\n",
    "            f.write('Why wrong? (2)for the Predicted class, positive node and edge are bad for Predicted class .\\n')\n",
    "            f.write('Visual reason:{}\\n'.format(visual_reason))\n",
    "            f.write('structure reason:{}\\n'.format(structure_reason))\n",
    "    '''\n",
    "    show all detected concept\n",
    "    '''\n",
    "    print('\\nGraph Prediction: {}\\n GT: {}'.format(norm_graph_predict, GT))\n",
    "\n",
    "    total_grad_out.clear()\n",
    "    total_activation_out.clear()\n",
    "    return reason_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4f4c1c-a017-4d12-a0cb-34e6bd055445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path):\n",
    "    f = open(path, 'r')\n",
    "    b = f.read()\n",
    "    pos_graph = eval(b)\n",
    "    f.close()\n",
    "    return pos_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475d57d2-b13d-4ae7-af61-1d8bddacac52",
   "metadata": {},
   "source": [
    "Make data package of graphs wrt positive and negative classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea6eea-4b22-470f-b3e8-d4f1ae3d40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(ori_graph_path, positive_class):\n",
    "    data_package = []\n",
    "    class_dict = {'fire_engine': 1, 'ambulance': 2, 'school_bus': 3}\n",
    "    class_list = ['fire_engine', 'ambulance', 'school_bus']\n",
    "    package_dict = {} # store info to load\n",
    "    package_dict[positive_class] = class_dict[positive_class] # add positive class and target label(eij label)\n",
    "    remain_cate = class_list.copy()\n",
    "    remain_cate.remove(positive_class)\n",
    "    for negative_class in remain_cate: # answer why not\n",
    "        package_dict[positive_class+'2'+negative_class] = class_dict[negative_class] # add negative label (eij label)\n",
    "\n",
    "    # package_dict: {\"fire_engine\":1, \"fire_engine2ambulance\":2, \"fire_engine2school_bus\":3}\n",
    "    for interest_class in package_dict.keys():\n",
    "        if interest_class == positive_class:\n",
    "            graph_path = ori_graph_path\n",
    "            '''XAI_activation'''\n",
    "            act_path = graph_path.replace('_graph.txt', '_XAI.npy')\n",
    "            act_vector = np.load(act_path)\n",
    "            # model_category_index = [279, 265, 962]  # fire_engine 279; ambulance 265; school_bus 962\n",
    "            model_category_index = [555, 407, 779]  # fire_engine 555; ambulance 407; school_bus 779 # Xception\n",
    "            XAI_act = act_vector  # all [5, 1008]\n",
    "            first_order_act = XAI_act[0]\n",
    "            interest_vec = []\n",
    "            for interest_dim in model_category_index:\n",
    "                interest_vec.append(first_order_act[interest_dim])\n",
    "            interest_vec_1 = torch.tensor(np.array(interest_vec)[np.newaxis, :])\n",
    "            interest_vec = f.normalize(interest_vec_1, p=1, dim=1)  # normalize\n",
    "            # interest_vec = softmax(interest_vec_1.cpu().detach().numpy())\n",
    "            data_package.append(interest_vec)\n",
    "        else:\n",
    "            graph_path = ori_graph_path.replace(positive_class, interest_class) # e.g. fire : fire2school, fire2ambu\n",
    "        '''\n",
    "        load data\n",
    "        '''\n",
    "        graph = read_txt(graph_path)\n",
    "        '''x, y'''\n",
    "        # Make graph representation for graph\n",
    "        x = torch.ones((len(graph), 2048))  # (4, 2048)\n",
    "        for n, vec in enumerate(graph.values()):\n",
    "            x[n] = torch.tensor(vec)\n",
    "        y = torch.FloatTensor([package_dict[interest_class]])  # label of each category\n",
    "        '''edge_index'''\n",
    "        source_nodes = []\n",
    "        target_nodes = []\n",
    "        start_choice = [n for n in range(len(graph))]\n",
    "        for startpt in start_choice:\n",
    "            # do not set loop\n",
    "            end_choice = start_choice.copy()\n",
    "            end_choice.remove(startpt)\n",
    "            for endpt in end_choice:\n",
    "                source_nodes.append(startpt)\n",
    "                target_nodes.append(endpt)\n",
    "        edge = torch.tensor([source_nodes, target_nodes], dtype=torch.long)\n",
    "\n",
    "        '''edge_feature'''\n",
    "        '''1 edge feature extraction'''\n",
    "        edge_path = graph_path.replace('_graph.', '_edge.')\n",
    "        edge_feature = read_txt(edge_path)\n",
    "        e_all = torch.zeros((len(edge_feature), 4, 4))  # (4, 4, 4)\n",
    "        for startpt, startpt_values in edge_feature.items():\n",
    "            for endpt, endpt_values in startpt_values.items():\n",
    "                e_all[startpt][endpt] = torch.tensor(endpt_values)\n",
    "        '''2 get edge_feature form edge_index'''\n",
    "        e = []\n",
    "        for index, source_p in enumerate(source_nodes):\n",
    "            target_p = target_nodes[index]\n",
    "            e.append(np.array(e_all[source_p][target_p]))\n",
    "        e = torch.Tensor(e)\n",
    "\n",
    "        # # merge the act into y label\n",
    "        data = Data(x=x, edge_attr=e, edge_index=edge, y=y)\n",
    "        img_path = graph_path.replace('_detect_graph', '_detect_img').replace('_graph.txt', '_img.png')\n",
    "        data_package.append((interest_class, img_path, data))\n",
    "\n",
    "    return data_package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd7964-be5f-49a2-b925-7d85473ab600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_locaton(data, img):\n",
    "    locations = []\n",
    "    y, x = img.shape[:2]\n",
    "    locations.append(data.edge_attr[0].cpu().numpy()[:2])\n",
    "    locations.append(data.edge_attr[0].cpu().numpy()[2:])\n",
    "    locations.append(data.edge_attr[1].cpu().numpy()[2:])\n",
    "    locations.append(data.edge_attr[2].cpu().numpy()[2:])\n",
    "    locations = [(int(loc[0]*x), int(loc[1]*y)) for loc in locations]\n",
    "    return locations\n",
    "\n",
    "\n",
    "def select_color(split_list, value):\n",
    "    if value <= split_list[0]:\n",
    "        color = 'red'\n",
    "    elif value <= split_list[1]:\n",
    "        color = 'orange'\n",
    "    elif value <= split_list[2]:\n",
    "        color = 'green'\n",
    "    else:\n",
    "        color = 'blue'\n",
    "    return color\n",
    "\n",
    "\n",
    "def load_image_from_file(filename, shape):\n",
    "    img = np.array(Image.open(filename).resize(\n",
    "        shape, Image.BILINEAR))\n",
    "    img = np.float32(img) / 255.0\n",
    "    if not (len(img.shape) == 3 and img.shape[2] == 3):\n",
    "        return None\n",
    "    else:\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c361758-38d4-4dde-9e61-9cf10ffa5513",
   "metadata": {},
   "outputs": [],
   "source": [
    "class show_concept():\n",
    "    def __init__(self, args):\n",
    "        # self.bottlenecks = 'block14_sepconv2_act'\n",
    "        self.data_path = args.result_root  # data path\n",
    "        self.source_dir = args.source_dir  # data path\n",
    "        self.model_path = args.model_path\n",
    "        self.img_size = (299, 299)\n",
    "        self.class2label = {'fire_engine': 1, 'ambulance': 2, 'school_bus': 3}\n",
    "        self.label2class = {'1': 'fire_engine', '2': 'ambulance', '3': 'school_bus'}\n",
    "        self.concept_dict = {\n",
    "            'ambulance': [18, 20, 11, 13],\n",
    "            'fire_engine': [8, 17, 16, 15],\n",
    "            'school_bus': [9, 3, 5, 19]}\n",
    "        self.cd_init()  # a dic contains all of cd we have, {name:cd}\n",
    "\n",
    "\n",
    "    def hook_fn_backward(self, module, grad_input, grad_output):  # get gradient\n",
    "        total_grad_out.append(grad_input)  # W * X + b\n",
    "\n",
    "    def hook_fn_forward(self, module, input, output):  # get activation of X\n",
    "        total_activation_out.append(input)\n",
    "\n",
    "    def cd_init(self):  # TBD have all CD knowledge\n",
    "        '''image init'''\n",
    "        model_to_run = 'Xception'\n",
    "        labels_path = 'src/Xception_labels.json'\n",
    "        sess = utils.create_session()\n",
    "        mymodel = ace_helpers.make_model(\n",
    "            None, model_to_run, None, labels_path)\n",
    "\n",
    "        self.cd = ConceptDiscovery(\n",
    "            mymodel,\n",
    "            target_class=\"\",\n",
    "            random_concept='random_concept',\n",
    "            bottlenecks='block14_sepconv2_act',\n",
    "            sess=sess,\n",
    "            source_dir=self.source_dir,\n",
    "            activation_dir=\"\",\n",
    "            cav_dir=\"\",\n",
    "            num_random_exp=20,\n",
    "            channel_mean=True,\n",
    "            max_imgs=40,\n",
    "            min_imgs=40,\n",
    "            num_discovery_imgs=40,\n",
    "            num_workers=0)\n",
    "\n",
    "    def cd_assign(self, class_name):\n",
    "        self.cd.target_class = class_name\n",
    "\n",
    "    def get_prior_knowledge(self, class_name):\n",
    "        prior_knowledge_path = os.path.join(self.data_path, 'output', class_name, 'concepts',\n",
    "                                            'all_concept_dict_X.txt')\n",
    "        with open(prior_knowledge_path, \"rb\") as fp:  # Pickling\n",
    "            prior_knowledge = pickle.load(fp)\n",
    "        return prior_knowledge\n",
    "\n",
    "\n",
    "    def show_concept(self, img, class_name, gradcam):\n",
    "        self.cd_assign(class_name)\n",
    "        prior_knowledge = self.get_prior_knowledge(class_name)\n",
    "        self.cd.create_patches(param_dict={'n_segments': [15, 50, 80]}, discovery_images=img[np.newaxis, :], gradcam=gradcam)\n",
    "        self.cd.match_dummy(prior=prior_knowledge, target = self.concept_dict[class_name] ,method='KM',\n",
    "                               param_dicts={'n_clusters': 25})\n",
    "\n",
    "        concept_images = []\n",
    "        target_concept = list(self.cd.center_match.keys())\n",
    "        for n, concept in enumerate(target_concept):\n",
    "            index = self.cd.center_match[concept]\n",
    "            concept_image = self.cd.dataset[index]\n",
    "            concept_images.append(concept_image)\n",
    "        return img, concept_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8a5c1f-9177-4fa3-bfcc-87826f9ab6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(image_path, reason_dict, data_package, args, class_list):\n",
    "    concept_score = []\n",
    "    edge_score = []\n",
    "    img = load_image_from_file(image_path, (299,299))\n",
    "    concept_class = show_concept(args)\n",
    "    for k, v in reason_dict.items():\n",
    "        loc_data = [loc[2] for loc in data_package[1:] if loc[0].split('2')[-1] == k][0]\n",
    "        reason_dict[k]['location'] = compute_locaton(loc_data, img)\n",
    "        edge_list = [abs(x) for x in v[\"structure_reason\"]]\n",
    "        reason_dict[k]['important_edge'] = heapq.nlargest(6, range(len(edge_list)), edge_list.__getitem__)\n",
    "        concept_score.extend(v[\"visual_reason\"])\n",
    "        edge_score.extend(v[\"structure_reason\"][[reason_dict[k]['important_edge']]])\n",
    "\n",
    "    split_concept = np.percentile(concept_score, (25, 50, 75), interpolation='midpoint')\n",
    "    split_edge = np.percentile(edge_score, (25, 50, 75), interpolation='midpoint')\n",
    "    edge_index = data_package[1][2]['edge_index'].cpu().numpy()\n",
    "\n",
    "    ball_size = 10\n",
    "    num_concepts, num = 4, 1\n",
    "    ground_truth = class_list[data_package[0].argmax()]\n",
    "    for idx, (k, v) in enumerate(reason_dict.items()):\n",
    "        print('Start plot the visualization for label %s...'%k)\n",
    "        concept_loc = v['location']\n",
    "        img, concept_images = concept_class.show_concept(img, k, ground_truth==k)\n",
    "\n",
    "        fig = plt.figure(figsize=(num * num_concepts, 5.5))\n",
    "        outer = gridspec.GridSpec(5, num_concepts)\n",
    "        for i in range(num_concepts):\n",
    "            color = select_color(split_concept, v[\"visual_reason\"][i])\n",
    "            ax = plt.Subplot(fig, outer[4, i])\n",
    "            ax.imshow(concept_images[i])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            title = 'concept {}'.format(i + 1)\n",
    "            ax.set_title(title, color=color,fontsize= ball_size)\n",
    "            ax.grid(False)\n",
    "            fig.add_subplot(ax)\n",
    "\n",
    "        ax = plt.Subplot(fig, outer[:4, :])\n",
    "        ax.imshow(img)\n",
    "        for i in range(len(concept_loc)):\n",
    "            color = select_color(split_concept, v[\"visual_reason\"][i])\n",
    "            ax.plot(concept_loc[i][0], concept_loc[i][1], 'o', color=color, markersize=ball_size,alpha=0.8)\n",
    "            ax.text(concept_loc[i][0], concept_loc[i][1], s=str(i+1), fontsize=ball_size, color='white', verticalalignment='center', horizontalalignment='center')\n",
    "        edges = v['important_edge']\n",
    "        for i in range(len(edges)):\n",
    "            color = select_color(split_edge, v[\"structure_reason\"][edges[i]])\n",
    "            index_list = edge_index[:, edges[i]]\n",
    "            # start_x = concept_loc[index_list[0]][0], concept_loc[index_list[1]][0]\n",
    "            # start_y = concept_loc[index_list[0]][1], concept_loc[index_list[1]][1]\n",
    "            # ax.plot(start_x, start_y, color=color, linewidth=4,alpha=0.7)\n",
    "            start = concept_loc[index_list[0]]\n",
    "            end = concept_loc[index_list[1]]\n",
    "            dx = abs(end[0]-start[0])\n",
    "            dy = abs(end[1]-start[1])\n",
    "            ddx = ball_size/(dx+dy)*dx * np.sign(end[0]-start[0])\n",
    "            ddy = ball_size/(dx+dy)*dy * np.sign(end[1]-start[1])\n",
    "            ax.arrow(start[0]+ddx, start[1]+ddy, end[0]-start[0]-2*ddx, end[1]-start[1]-2*ddy, head_width=ball_size, linewidth=int(ball_size/3),fc=color,ec=color,length_includes_head=True,alpha=0.8)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if args.img_class == k:\n",
    "            title = 'why ' + k.replace('_', ' ')\n",
    "        else:\n",
    "            title = 'why not ' + k.replace('_', ' ')\n",
    "        ax.set_title(title)\n",
    "        ax.grid(False)\n",
    "        fig.add_subplot(ax)\n",
    "        plt.show()\n",
    "\n",
    "        with tf.gfile.Open(os.path.join(args.result_root, \"reason_results\", \"%s2%s_%s.png\"%(args.img_class, k, args.img_idx)), 'w') as f:\n",
    "            fig.savefig(f,dpi=600)\n",
    "\n",
    "    fig = plt.figure(figsize=(13, 6.5))\n",
    "    outer = gridspec.GridSpec(1, 3, wspace=0.00001, hspace=0.00001)\n",
    "    for idx, (k, v) in enumerate(reason_dict.items()):\n",
    "        ax = plt.Subplot(fig, outer[idx])\n",
    "        img_path = os.path.join(args.result_root, \"reason_results\", \"%s2%s_%s.png\"%(args.img_class, k, args.img_idx))\n",
    "        img = mpimg.imread(img_path)\n",
    "        ax.imshow(img)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.grid(False)\n",
    "        fig.add_subplot(ax)\n",
    "        plt.axis('off')\n",
    "        os.remove(img_path)\n",
    "    fig.add_axes([0.87, 0.15, 0.05, 0.7],frameon=False)\n",
    "    plt.axis('off')\n",
    "    cb = fig.colorbar(matplotlib.cm.ScalarMappable(cmap=\"jet\"))\n",
    "    cb.set_ticks([])\n",
    "    cb.update_ticks()\n",
    "    plt.text(0.80, 0.20, s='Positive', fontsize=ball_size)\n",
    "    plt.text(0.80, 0.78, s='Negative', fontsize=ball_size)\n",
    "    plt.show()\n",
    "    with tf.gfile.Open(os.path.join(args.result_root, \"reason_results\", \"%s_%s.png\" % (args.img_class, args.img_idx)), 'w') as f:\n",
    "        fig.savefig(f, pad_inches = 0, bbox_inches = 'tight')\n",
    "\n",
    "    print('Finished analyze! You can find the output files at %s' % os.path.join(args.result_root, \"reason_results\"))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a844e46e-897b-47c0-8306-6243c8bb1d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    '''\n",
    "    testing dataset may be same as training\n",
    "    '''\n",
    "    os.makedirs(os.path.join(args.result_root, \"reason_results\"), exist_ok=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model_share = MyGCNNet_shareW_adap_batch_withhook(2048, interest_class_num=3)  # all class share base structure\n",
    "    '''\n",
    "    we will use the trained model\n",
    "    '''\n",
    "    model_share.load_state_dict(torch.load(args.model_path, map_location=device))\n",
    "\n",
    "    model_share = model_share.to(device)\n",
    "    model_share.batch_size = 1 # fix the batch size\n",
    "    ''' load pretrained model'''\n",
    "    print('[Pre] 1 Loading class-specific model')\n",
    "\n",
    "    '''register hook for reasoning'''\n",
    "    # hook will be called every time after forward()/backward() has computed an output\n",
    "    model_share.fc2.register_forward_hook(hook_fn_forward)\n",
    "    model_share.fc2.register_backward_hook(hook_fn_backward)\n",
    "\n",
    "    '''Start reasoning'''\n",
    "    image_path = os.path.join(args.source_dir, args.img_class, \"%s_%s.JPEG\"%(class_dict[args.img_class], args.img_idx))\n",
    "    graph_path = os.path.join(args.result_root, 'img2vec', args.img_class+'_detect_graph',\n",
    "                              '{}_{}_graph.txt'.format(class_dict[args.img_class], args.img_idx))\n",
    "    data_package = select_data(graph_path, args.img_class)  # contain both positive and negative graph\n",
    "    reason_dict = reasoning(model_share, data_package, device, args)\n",
    "    visualize_results(image_path, reason_dict, data_package, args, class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e53751-7c53-466d-b6e7-50556e72e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--source_dir', type=str, default='source',\n",
    "      help='''Directory where the network's classes image folders and random concept folders are saved.''')\n",
    "    parser.add_argument('--result_root', type=str, help='directory to results of discover concept.py.',\n",
    "                      default='result')\n",
    "    parser.add_argument('--img_class', type=str, help='the class of image you want to test', default='fire_engine')\n",
    "    parser.add_argument('--img_idx', type=str, help='the idx of image you want to test', default=\"19835\")\n",
    "    parser.add_argument('--model_path', type=str,\n",
    "      help='Path to model checkpoints.', default='src/GraphConv_Xception_model.pt')\n",
    "    return parser.parse_args(argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438aba1-f88e-4d6f-9589-6130887e6283",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    total_grad_out = []\n",
    "    total_activation_out = []\n",
    "\n",
    "    class_dict = {\n",
    "        'ambulance': 'n02701002',\n",
    "        'fire_engine': 'n03345487',\n",
    "        'school_bus': 'n04146614'\n",
    "    }\n",
    "    class_list = ['fire_engine', 'ambulance', 'school_bus']\n",
    "\n",
    "    main(parse_arguments(sys.argv[1:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
